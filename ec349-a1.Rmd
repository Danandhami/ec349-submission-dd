---
title: "Assignment ec349"
author: "u2103494"
date: "2023-12-05"
output: 
 html_document:
  theme: sandstone
  toc: true
  toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(eval=FALSE,echo = TRUE)
```

## Introduction

We were tasked with using data from 5 Yelp datasets - containing review, user, business, check-in and tip data - to predict users' ratings of businesses. 

After cleaning the data, I ran a random forest classifier trained on 315,000 observations to predict the stars given by users in 35000 randomly drawn observations with almost 60% accuracy, with the suggestion that the average stars given by users is the most important variable included.

A limitation of the project is the lack of sentiment analysis, losing potentially significant information given by review text (omitted due to slow computing time).

A key difficulty in predicting the review a user will give is distinguishing between moderate reviews (i.e 2,3 and 4 star reviews). More extreme reviews (1 and 5 star) are more common and have stronger relationships with most variables.

I will note my decision to disable the running of code chunks in this markdown file, due to the time required to run my code. I will highlight certain codes here, and include some outputs. The associated script is available to run.

## Data Cleaning

```{r the datasets}
setwd("C:/Users/u2103494/OneDrive - University of Warwick/ec349")
load("yelp_review_small (1).Rda")
require(jsonlite)
businessdata <- stream_in(file("yelp_academic_dataset_business.json"))
checkindata <- stream_in(file("yelp_academic_dataset_checkin.json"))
tipdata <- stream_in(file("yelp_academic_dataset_tip.json"))
userdata <- stream_in(file("yelp_academic_dataset_user.json"))
```
Before running any classification models, we need to try to extract all the relevant information we can from our data, and remove inconsistencies.

The data cleaning process was mostly simple, involving ensuring existing variables were stored as the correct data type and deriving new variables which could help classify reviews. In addition to the pre-processing of data I outline here, I tried to apply VADER sentiment analysis to the text, but did not yield results.

Dealing with user data first, I focused on extracting some new variables. Firstly, I took the year in which a user joined the platform from the first 4 letters of the yelping_since column, to then use in conjunction with review date to see if length of time on the platform might help classify reviews.

```{r date of joining}
minuserdata <- select(userdata,user_id,review_count,yelping_since,useful,funny,cool,fans,average_stars,elite,friends)
minuserdata <- minuserdata %>%
  #changing the yelping_since to a numerical variable representing year
  mutate(startyear=substr(yelping_since,1,4))%>%
  mutate(startyear=as.numeric(startyear))
```

An interesting variable was "elite" - showing the years when a user was classed as such. I worked around some structural errors in the data, to construct a variable representing the number of years a user was elite.

```{constructing yearselite}
minuserdata <- minuserdata %>%
  mutate(elite=ifelse(elite=="",NA,elite))%>%
  mutate(elitedummy=ifelse(!is.na(elite),1,0))%>%
  mutate(elite=str_replace(elite,"20,20,2021","2020,2021"))%>%
  mutate(elite=str_replace(elite,"20,20","2020"))%>%
  mutate(elite=str_replace(elite,"20202021","2020,2021"))%>%
  mutate(yearselite=ifelse(elitedummy ==1, str_count(elite,",")+1,0))%>%
  
```

I also found check_in data interesting and potentially useful, as it can in some way represent the popularity of businesses. So after some cleaning I generated a variable representing the number of check ins a business has had, and merged the data with the business dataset.

```{r adding check in number to businesses}
checkin1 <- checkindata%>%
  #counting the number of checkins - may be relevant to customer opinion
  mutate(checkincount=str_count(date,",")+1)
checkin1 <- subset(checkin1, select = c(business_id,checkincount))
rm(checkindata)

#matching the number of checkings to the business. Unmatched businesses receive n/a 
minbusinessdata <- left_join(businessdata,checkin1,by = "business_id")
```
Business data itself had a large number of variables, many representing business attributes. However, after checking the proportion of rows populated by n/a, I decided only to use accepts credit cards out of the attributes, as others were mostly n/a.
```{r cleaning accepts credit card}

minbusinessdata <- minbusinessdata%>%
  #replacing n/a in check in count with 0 - as no match implies no check in data for that business
  mutate(checkincount=ifelse(is.na(checkincount),0,checkincount))%>%
  mutate(creditcards=ifelse(attributes$BusinessAcceptsCreditCards=="None","False",attributes$BusinessAcceptsCreditCards))%>%
  mutate(creditcards=ifelse(creditcards=="True",1,ifelse(creditcards=="False",0,creditcards)))%>%
  mutate(creditcards=as.numeric(creditcards))%>%
  #after summarising the column, the mean value is 0.949 - so mode of 1 imposed on n/as
  mutate(creditcards=ifelse(is.na(creditcards),1,creditcards))
```

After some other data cleaning, I cut down the review observations randomly, to lessen the burden on my computer when running later processes, and merged all my data into a final dataframe.
```{r making the dataframe}
set.seed(1)
smallerrevdata <-  revdata%>%sample_n(size=350000,replace=FALSE)
smallerrevdata <- select(smallerrevdata,-c(date))

merged1 <- left_join(smallerrevdata,minuserdata,by="user_id")
merged2 <- left_join(merged1,minbusinessdata1,by="business_id")
rm(merged1)
rm(minbusinessdata1)
rm(revdata)
rm(smallerrevdata)
rm(minuserdata)
##making stars categorical, removing elitedummy, getting yelpyears, 
merged2<-merged2%>%
  mutate(stars=as.factor(stars))%>%
  mutate(yelpyears=reviewyear-startyear)

#drop id data and drop startyear, although multicolinearity shouldn't be an issue for random forest
merged2<-select(merged2,-c(review_id,user_id,business_id,elitedummy,startyear))


####FOR NOW - EXCLUDING TEXT - No sentiment analysis intitally.
merged3<-select(merged2,-c(text))
```

## Plotting the Data

Now that we have our dataframe, I deemed it useful to plot some of our variables in relation to the stars given.

I plotted the distribution of observaitions across the star categories
```{r star frequency plot}
require(ggplot2)
require(GGally)
star_counts <- table(merged4$stars)
percentage <- prop.table(star_counts) * 100  

percentage_df <- data.frame(Stars = names(percentage), Percentage = as.numeric(percentage))


ggplot(percentage_df, aes(x = Stars, y = Percentage)) +
  geom_bar(stat = "identity", fill = "skyblue", color = "black") +
  labs(x = "Stars", y = "Percentage of Observations") +
  ggtitle("Percentage of Observations in Each Star Category")
```
![Alt text](C:/Users/danan/Documents/3rd Year Uni/Modules/Ec349 Data Science/Assignment 1/plots/proportionstarsplot.png)
We can see that proportionately there are far more 5 star reviews and less moterate reviews (2 and 3 stars).

![Alt text](C:/Users\danan\Documents\3rd Year Uni\Modules\Ec349 Data Science\Assignment 1\plots\boxplotsforusefulstars.png)
![Alt text](C:/Users\danan\Documents\3rd Year Uni\Modules\Ec349 Data Science\Assignment 1\plots\businessanduseraverageplot.png)
![Alt text](C:/Users\danan\Documents\3rd Year Uni\Modules\Ec349 Data Science\Assignment 1\plots\yelpyears and elite plot.png)
![Alt text](C:/Users\danan\Documents\3rd Year Uni\Modules\Ec349 Data Science\Assignment 1\plots\reviewnoplot.png)
Plotting some of the variables against what we want to predict - the stars given in a review - show that some variables do seem to hold some information which could be useful for classification. In particular, the average stars given by a user and the average stars of a business might be useful. Some of my newly constructed variables such as check in number seem to also be useful.

## Chosen Classifier - Random Forest

I decided to use a random forest classifier, since our task was to classify stars given in reviews (which in this data took discrete values 1 to 5) and so the loss of interpretability in comparison to linear models is not so important.

Random forest is one of the more popular machine learning methods, and it uses bootstrapping, alongside random sampling of predictors to generate a forest of decorrelated decision trees, from which an average classification can be made.

The method is superior in reducing variance compared to bagging and is not as sensitive to outliers as other methods (James et al, 2021) - which is appealing given I found that many variables had outliers when plotted.

Although the model isn't as easy to interpret as linear models, the out-of-bag error estimate provides a straightforward way to estimate our model's error, and variable importance can be quantified - allowing some interpretation.

## Running the Model

Initially, I ran a model with 500 trees, and set the number of predictors allowed at each split at 5.
```{r initial random forest}
set.seed(230)
require(randomForest)
rf1<-randomForest(stars ~ ., data=trainingdata, ntree=500, mtry=5,do.trace=TRUE)
plot(rf1, main="Error Rate vs. Number of Trees, mtry=5", type="l", col="blue")
```

![Alt text](C:/Users\danan\Documents\3rd Year Uni\Modules\Ec349 Data Science\Assignment 1\plots\errors of 5 mtry 500 tree.png)
By plotting the OOB error (the third line from the bottom) against the number of trees, we can see that the OOB error stabilises quite early, so I took 350 trees as a reasonable number moving forward. We may notice however, that the error for predicting 2-star and 3-star reviews is very high (the highest two lines)


To tune my random forest model, I ran an iteration through mtry=1 to mtry=8 (denoting the number of variables selected for a split). I only ran each model with 50 trees to reduce the instensity of the program.
```{r tuning the parameters}
oob.err<-rep(0,8)
#test.err-double(8)
for(j in 1:8){
  fit<-randomForest(stars~.,data=trainingdata, ntree=50, mtry=j,do.trace=TRUE) *
    oob.err[j]<-fit$err.rate[50,1]
    #pred-predict(fit,testdata)
    #test.rr[mtry]-with(testdata,mean((medv-pred)^2))
    cat(j," ")
}

plot(1:8, oob.err, type = "b", xlab = "mtry", ylab = "Out-of-bag Error",
     main = "Out-of-Bag Error vs. mtry Values")
```

![Alt text](C:/Users\danan\Documents\3rd Year Uni\Modules\Ec349 Data Science\Assignment 1\plots\outofbagerror vs mtry over ntree 50.png)
By plotting the OOB against different mtry values, we can see that there is no evidence that having more than 4 selected predictors would help the model signfiicantly.


I thus ran my model with my tuned paramters, mtry=4 and ntree=350
```{r model with tuned parameters}
#optimal random forest seems to be mtry=3 to 4 and ntree=350 so now running the optimal model
rf2<-randomForest(stars ~ ., data=trainingdata, ntree=350, mtry=4,do.trace=TRUE)
rf2prediction <- predict(rf2,testdata,type="class")
```

The resultant out of bag error was 40.72% and accuracy on the training data was 0.5928.

**OOB error: 40.72%**
```{r create_table, eval=TRUE, echo=FALSE}
# Create a data frame with the provided values
data <- data.frame(
  `1` = c(34477, 7486, 3480, 3079, 5607),
  `2` = c(936, 2415, 809, 705, 611),
  `3` = c(849, 1564, 3383, 3018, 1397),
  `4` = c(2459, 4554, 10725, 23578, 14797),
  `5` = c(9512, 8517, 12670, 35484, 122887),
  `class.error` = c(0.2851989, 0.9015732, 0.8911063, 0.6420199, 0.1542474)
)

# Display the table
knitr::kable(data)
```
**Accuracy on training = 0.5928**


```{r confusion matrix, eval=FALSE}
confusionmatrix<-table(actual=testdata$stars,predicted=rf2prediction)
```

Running the trained model over the test data will then determine how well we have classified stars in reviews.

```{r createtable, eval=TRUE, echo=FALSE}
# Create a data frame with the provided values
data2 <- data.frame(
  `1` = c(3902, 799, 399, 337, 653),
  `2` = c(91, 239, 73, 70, 75),
  `3` = c(93, 155, 370, 313, 145),
  `4` = c(293, 528, 1154, 2737, 3925),
  `5` = c(1061, 937, 1457, 1608, 13586)
  
)

# Display the table
knitr::kable(data2)
```


```{r accuracy}
accuracy <- (sum(diag(confusionmatrix)))/(sum(confusionmatrix))
print(accuracy)
```
**Accuracy on test = 0.5952571**

We have a very similar accuracy compared with the training data.

Finally, we can plot the variance importance, measured by mean reduction in Gini impurity, giving some indication of the contribution each variable has to classifying the data.

```{r variable importance}
varImpPlot(rf2)
```
![Alt text](C:/Users\danan\Documents\3rd Year Uni\Modules\Ec349 Data Science\Assignment 1\plots\variable importance .png)

## Evaluation

My random forest model fit the training and test data similarly, with around 60% accuracy, although the slightly higher observed accuracy in predicting  the test data may suggest I did not include all relevant variables, in particular, I was not successful in including any text analysis. 

We may also notice that, as expected, classifiction was more difficult for moderate reviews compared with 1-star and 5-star - which were predicted well. This limitation may also be combatted by constructing different variables which can be included in our classification model.

We see that the average stars of each user might be the most informative variable in classifying an individual review, and the number of check ins to a business was also useful. Interstingly we see that years a user has been elite isn't necessarily a useful predictor in classifying reviews, and that the one business attribute I included was the least important.

## References
James, G.; Witten, D.; Hastie, T.; & Tibshirani, R. 2021 (2nd Ed.). An Introduction to Statistical Learning with Applications in R. Springer.